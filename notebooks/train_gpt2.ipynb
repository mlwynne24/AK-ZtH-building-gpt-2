{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5804b9ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tiktoken in /usr/local/lib/python3.11/dist-packages (0.9.0)\n",
      "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.11/dist-packages (from tiktoken) (2024.11.6)\n",
      "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.11/dist-packages (from tiktoken) (2.32.4)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken) (2025.6.15)\n"
     ]
    }
   ],
   "source": [
    "!pip install tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68728985",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle\n"
     ]
    }
   ],
   "source": [
    "cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0c8bb964",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using device: cuda\n",
      "loaded 338025 tokens\n",
      "1 epoch = 165 batches\n",
      "step 0, loss: 10.934581756591797, dt: 9972.41ms, tok/sec: 205.37\n",
      "step 1, loss: 9.472172737121582, dt: 568.39ms, tok/sec: 3603.15\n",
      "step 2, loss: 9.200077056884766, dt: 564.38ms, tok/sec: 3628.75\n",
      "step 3, loss: 8.896628379821777, dt: 570.45ms, tok/sec: 3590.14\n",
      "step 4, loss: 8.751442909240723, dt: 573.28ms, tok/sec: 3572.44\n",
      "step 5, loss: 8.46125602722168, dt: 573.99ms, tok/sec: 3568.00\n",
      "step 6, loss: 8.128143310546875, dt: 572.94ms, tok/sec: 3574.54\n",
      "step 7, loss: 7.980914115905762, dt: 576.64ms, tok/sec: 3551.62\n",
      "step 8, loss: 7.6942315101623535, dt: 575.92ms, tok/sec: 3556.04\n",
      "step 9, loss: 7.5070600509643555, dt: 577.68ms, tok/sec: 3545.20\n",
      "step 10, loss: 7.310483455657959, dt: 578.02ms, tok/sec: 3543.14\n",
      "step 11, loss: 6.877572059631348, dt: 582.39ms, tok/sec: 3516.57\n",
      "step 12, loss: 6.932930946350098, dt: 576.86ms, tok/sec: 3550.24\n",
      "step 13, loss: 6.809874534606934, dt: 582.17ms, tok/sec: 3517.86\n",
      "step 14, loss: 6.771484375, dt: 584.09ms, tok/sec: 3506.30\n",
      "step 15, loss: 6.586923599243164, dt: 582.41ms, tok/sec: 3516.44\n",
      "step 16, loss: 6.822099685668945, dt: 585.13ms, tok/sec: 3500.07\n",
      "step 17, loss: 6.476125240325928, dt: 586.35ms, tok/sec: 3492.77\n",
      "step 18, loss: 6.389767646789551, dt: 585.80ms, tok/sec: 3496.10\n",
      "step 19, loss: 6.328987121582031, dt: 589.62ms, tok/sec: 3473.42\n",
      "step 20, loss: 6.58715295791626, dt: 589.05ms, tok/sec: 3476.81\n",
      "step 21, loss: 6.480066299438477, dt: 584.98ms, tok/sec: 3500.95\n",
      "step 22, loss: 6.495859146118164, dt: 588.84ms, tok/sec: 3478.01\n",
      "step 23, loss: 7.137153148651123, dt: 591.68ms, tok/sec: 3461.34\n",
      "step 24, loss: 7.274115562438965, dt: 589.71ms, tok/sec: 3472.89\n",
      "step 25, loss: 7.089376926422119, dt: 590.07ms, tok/sec: 3470.79\n",
      "step 26, loss: 7.10960578918457, dt: 592.23ms, tok/sec: 3458.11\n",
      "step 27, loss: 7.138219833374023, dt: 594.22ms, tok/sec: 3446.56\n",
      "step 28, loss: 6.944033622741699, dt: 592.39ms, tok/sec: 3457.16\n",
      "step 29, loss: 6.66181755065918, dt: 592.01ms, tok/sec: 3459.42\n",
      "step 30, loss: 6.815823078155518, dt: 594.01ms, tok/sec: 3447.77\n",
      "step 31, loss: 6.9006028175354, dt: 597.20ms, tok/sec: 3429.32\n",
      "step 32, loss: 6.836899757385254, dt: 595.58ms, tok/sec: 3438.64\n",
      "step 33, loss: 6.713947296142578, dt: 594.70ms, tok/sec: 3443.73\n",
      "step 34, loss: 6.560980319976807, dt: 597.53ms, tok/sec: 3427.43\n",
      "step 35, loss: 6.498556137084961, dt: 599.44ms, tok/sec: 3416.53\n",
      "step 36, loss: 6.7737016677856445, dt: 599.06ms, tok/sec: 3418.67\n",
      "step 37, loss: 6.65411901473999, dt: 602.58ms, tok/sec: 3398.73\n",
      "step 38, loss: 6.599599838256836, dt: 599.20ms, tok/sec: 3417.87\n",
      "step 39, loss: 6.83477783203125, dt: 602.04ms, tok/sec: 3401.79\n",
      "step 40, loss: 6.946971416473389, dt: 601.12ms, tok/sec: 3406.98\n",
      "step 41, loss: 6.897885322570801, dt: 604.32ms, tok/sec: 3388.96\n",
      "step 42, loss: 6.7507781982421875, dt: 603.01ms, tok/sec: 3396.27\n",
      "step 43, loss: 6.617123603820801, dt: 607.02ms, tok/sec: 3373.86\n",
      "step 44, loss: 6.753781318664551, dt: 605.28ms, tok/sec: 3383.57\n",
      "step 45, loss: 6.618504524230957, dt: 608.84ms, tok/sec: 3363.80\n",
      "step 46, loss: 6.696441650390625, dt: 606.24ms, tok/sec: 3378.23\n",
      "step 47, loss: 6.823442459106445, dt: 608.40ms, tok/sec: 3366.19\n",
      "step 48, loss: 6.734795570373535, dt: 606.48ms, tok/sec: 3376.88\n",
      "step 49, loss: 6.845853805541992, dt: 609.02ms, tok/sec: 3362.78\n"
     ]
    }
   ],
   "source": [
    "from dataclasses import dataclass\n",
    "import math\n",
    "import time\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "import tiktoken\n",
    "\n",
    "# ---------------------------------------------------------------\n",
    "\n",
    "@dataclass\n",
    "class GPTConfig:\n",
    "    block_size: int = 1024  # max sequence length\n",
    "    vocab_size: int = 50257  # number of tokens: 50,000 BPE merges + 256 bytes tokens + 1 <|endoftext|> token\n",
    "    n_layer: int = 12  # number of layers\n",
    "    n_head: int = 12  # number of heads\n",
    "    n_embd: int = 768  # embedding dimension\n",
    "\n",
    "\n",
    "class CausalSelfAttention(nn.Module):\n",
    "    def __init__(self, config: GPTConfig):\n",
    "        super().__init__()\n",
    "        assert config.n_embd % config.n_head == 0\n",
    "        # key, query, value projections for all heads but in a batch\n",
    "        self.c_attn = nn.Linear(config.n_embd, 3 * config.n_embd)\n",
    "        # output projection\n",
    "        self.c_proj = nn.Linear(config.n_embd, config.n_embd)\n",
    "        self.c_proj.NANOGPT_SCALE_INIT = 1\n",
    "        # regularization\n",
    "        self.n_head = config.n_head\n",
    "        self.n_embd = config.n_embd\n",
    "        # buffer - not really bias, more a mask\n",
    "        self.register_buffer(\n",
    "            \"bias\",\n",
    "            (\n",
    "                torch.tril(torch.ones(config.block_size, config.block_size)).view(\n",
    "                    1, 1, config.block_size, config.block_size\n",
    "                )\n",
    "            ),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C = (\n",
    "            x.size()\n",
    "        )  # batch size, sequence length, embedding dimensionality (n_embd)\n",
    "        # calculate q, k, v for all heads in batch and move head forward to be the batch dimension\n",
    "        # nh is \"number of heads\", hs is \"head size\", and C (number of channels) = nh * hs\n",
    "        # e.g. in GPT-2 (124M), n_head = 12, hs=64, so nh*hs=C=768 channels in the Transformer\n",
    "        qkv = self.c_attn(x)\n",
    "        q, k, v = qkv.split(self.n_embd, dim=2)\n",
    "        k = k.view(B, T, self.n_head, C // self.n_head).transpose(\n",
    "            1, 2\n",
    "        )  # (B, nh, T, hs)\n",
    "        q = q.view(B, T, self.n_head, C // self.n_head).transpose(\n",
    "            1, 2\n",
    "        )  # (B, nh, T, hs)\n",
    "        v = v.view(B, T, self.n_head, C // self.n_head).transpose(\n",
    "            1, 2\n",
    "        )  # (B, nh, T, hs)\n",
    "        # attention (materializes the large (T, T) matrix for all the queries and keys)\n",
    "        att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.size(-1)))\n",
    "        att = att.masked_fill(self.bias[:, :, :T, :T] == 0, float(\"-inf\"))\n",
    "        att = F.softmax(att, dim=-1)\n",
    "        y = att @ v  # (B, nh, T, T) x (B, nh, T, hs) -> (B, nh, T, hs)\n",
    "        y = (\n",
    "            y.transpose(1, 2).contiguous().view(B, T, C)\n",
    "        )  # reassemble all head outputs side by side\n",
    "        y = self.c_proj(y)\n",
    "        return y\n",
    "\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, config: GPTConfig):\n",
    "        super().__init__()\n",
    "        self.c_fc = nn.Linear(config.n_embd, 4 * config.n_embd)\n",
    "        self.gelu = nn.GELU(approximate=\"tanh\")\n",
    "        self.c_proj = nn.Linear(4 * config.n_embd, config.n_embd)\n",
    "        self.c_proj.NANOGPT_SCALE_INIT = 1\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.c_fc(x)\n",
    "        x = self.gelu(x)\n",
    "        x = self.c_proj(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class Block(nn.Module):\n",
    "    def __init__(self, config: GPTConfig):\n",
    "        super().__init__()\n",
    "        self.ln_1 = nn.LayerNorm(config.n_embd)\n",
    "        self.attn = CausalSelfAttention(config)\n",
    "        self.ln_2 = nn.LayerNorm(config.n_embd)\n",
    "        self.mlp = MLP(config)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.attn(self.ln_1(x))\n",
    "        x = x + self.mlp(self.ln_2(x))\n",
    "        return x\n",
    "\n",
    "\n",
    "class GPT2(nn.Module):\n",
    "    def __init__(self, config: GPTConfig):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "\n",
    "        self.transformer = nn.ModuleDict(\n",
    "            dict(\n",
    "                wte=nn.Embedding(config.vocab_size, config.n_embd),\n",
    "                wpe=nn.Embedding(config.block_size, config.n_embd),\n",
    "                h=nn.ModuleList([Block(config) for _ in range(config.n_layer)]),\n",
    "                ln_f=nn.LayerNorm(config.n_embd),\n",
    "            )\n",
    "        )\n",
    "        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)\n",
    "\n",
    "        # weight-sharing scheme\n",
    "        self.transformer.wte.weight = self.lm_head.weight\n",
    "\n",
    "        # init params\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            std = 0.02\n",
    "            if hasattr(module, \"NANOGPT_SCALE_INIT\"):\n",
    "                std *= (2 * self.config.n_layer) ** -0.5\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=std)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "\n",
    "    def forward(self, idx: torch.Tensor, targets: torch.Tensor = None):\n",
    "        # idx is of shape (B, T)\n",
    "        B, T = idx.size()\n",
    "        assert (\n",
    "            T <= self.config.block_size\n",
    "        ), f\"Cannot forward sequence of length {T}, block size is {self.config.block_size}\"\n",
    "        # forward the token and position embeddings\n",
    "        pos = torch.arange(0, T, dtype=torch.long, device=idx.device)  # shape (T)\n",
    "        pos_emb = self.transformer.wpe(pos)  # position embeddings of shape (T, n_embd)\n",
    "        tok_emb = self.transformer.wte(idx)  # token embeddings of shape (B, T, n_embd)\n",
    "        x = tok_emb + pos_emb\n",
    "        # forward the blocks of the transformer\n",
    "        for block in self.transformer.h:\n",
    "            x = block(x)\n",
    "        # forward the final layernorm and the classifier\n",
    "        x = self.transformer.ln_f(x)\n",
    "        logits = self.lm_head(x)  # (B, T, vocab_size)\n",
    "        loss = None\n",
    "        if targets is not None:\n",
    "            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1))\n",
    "        return logits, loss\n",
    "\n",
    "    @classmethod\n",
    "    def from_pretrained(cls, model_type) -> \"GPT2\":\n",
    "        \"\"\"Loads pretrained GPT-2 model weights from huggingface\"\"\"\n",
    "        assert model_type in {\"gpt2\", \"gpt2-medium\", \"gpt2-large\", \"gpt2-xl\"}\n",
    "        from transformers import GPT2LMHeadModel\n",
    "\n",
    "        print(f\"loading weights from pretrained gpt: {model_type}\")\n",
    "\n",
    "        # n_layer, n_head and n_embd are determined from model_type\n",
    "        config_args = {\n",
    "            \"gpt2\": dict(n_layer=12, n_head=12, n_embd=768),  # 124M params\n",
    "            \"gpt2-medium\": dict(n_layer=24, n_head=16, n_embd=1_024),  # 350M params\n",
    "            \"gpt2-large\": dict(n_layer=36, n_head=20, n_embd=1_280),  # 774M params\n",
    "            \"gpt2-xl\": dict(n_layer=48, n_head=25, n_embd=1_600),  # 1558M params\n",
    "        }[model_type]\n",
    "        config_args[\"vocab_size\"] = 50_257\n",
    "        config_args[\"block_size\"] = 1_024\n",
    "        # create a from-scratch initialised minGPT model\n",
    "        config = GPTConfig(**config_args)\n",
    "        model = GPT2(config)\n",
    "        sd = model.state_dict()\n",
    "        sd_keys = sd.keys()\n",
    "        sd_keys = [\n",
    "            k for k in sd_keys if not k.endswith(\".attn.bias\")\n",
    "        ]  # discard this mask / buffer\n",
    "\n",
    "        # init a huggingface/transformers model\n",
    "        model_hf = GPT2LMHeadModel.from_pretrained(model_type)\n",
    "        sd_hf = model_hf.state_dict()\n",
    "\n",
    "        # copy while ensuring all fo the parameters are aligned and match in names and shapes\n",
    "        sd_keys_hf = sd_hf.keys()\n",
    "        sd_keys_hf = [\n",
    "            k for k in sd_keys_hf if not k.endswith(\".attn.masked_bias\")\n",
    "        ]  # ignore, just a buffer\n",
    "        sd_keys_hf = [k for k in sd_keys_hf if not k.endswith(\".attn.bias\")]  # same\n",
    "        transposed = [\n",
    "            \"attn.c_attn.weight\",\n",
    "            \"attn.c_proj.weight\",\n",
    "            \"mlp.c_fc.weight\",\n",
    "            \"mlp.c_proj.weight\",\n",
    "        ]\n",
    "        # basically the openai checkpoints use a \"Conv1D\" module, but we only want to use a vanilla Linear\n",
    "        # this means that we have to transpose these weights when we import them\n",
    "        assert len(sd_keys_hf) == len(\n",
    "            sd_keys\n",
    "        ), f\"mismatched keys: {len(sd_keys_hf)} != {len(sd_keys)}\"\n",
    "        for k in sd_keys_hf:\n",
    "            if any(k.endswith(w) for w in transposed):\n",
    "                # special treatment for the Conv1D weights we need to transpose\n",
    "                assert sd_hf[k].shape[::-1] == sd[k].shape\n",
    "                with torch.no_grad():\n",
    "                    sd[k].copy_(sd_hf[k].t())\n",
    "            else:\n",
    "                # vanilla copy over the other parameters\n",
    "                assert sd_hf[k].shape == sd[k].shape\n",
    "                with torch.no_grad():\n",
    "                    sd[k].copy_(sd_hf[k])\n",
    "\n",
    "        return model\n",
    "\n",
    "\n",
    "# --------------------------------------------------------------\n",
    "\n",
    "\n",
    "class DataLoaderLite:\n",
    "    def __init__(self, B, T):\n",
    "        self.B = B\n",
    "        self.T = T\n",
    "\n",
    "        # at init load tokens from disk and store them in memory\n",
    "        with open(\"input/input-txt/input.txt\", \"r\") as f:\n",
    "            text = f.read()\n",
    "        enc = tiktoken.get_encoding(\"gpt2\")\n",
    "        tokens = enc.encode(text)\n",
    "        self.tokens = torch.tensor(tokens)\n",
    "        print(f\"loaded {len(self.tokens)} tokens\")\n",
    "        print(f\"1 epoch = {len(self.tokens) // (B * T)} batches\")\n",
    "\n",
    "        # state\n",
    "        self.current_position = 0\n",
    "\n",
    "    def next_batch(self):\n",
    "        B, T = self.B, self.T\n",
    "        buf = self.tokens[self.current_position : self.current_position + B * T + 1]\n",
    "        x = (buf[:-1]).view(B, T)  # inputs\n",
    "        y = (buf[1:]).view(B, T)  # targets\n",
    "        # advance the position in the tensor\n",
    "        self.current_position += B * T\n",
    "        # if loading the next batch would be out of bounds, reset\n",
    "        if self.current_position + (B * T + 1) > len(self.tokens):\n",
    "            self.current_position = 0\n",
    "        return x, y\n",
    "\n",
    "\n",
    "# --------------------------------------------------------------\n",
    "# attempt to autodetect the device\n",
    "device = \"cpu\"\n",
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "elif hasattr(torch.backends, \"mps\") and torch.backends.mps.is_available():\n",
    "    device = \"mps\"\n",
    "print(f\"using device: {device}\")\n",
    "\n",
    "torch.manual_seed(1337)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(1337)\n",
    "\n",
    "train_loader = DataLoaderLite(B=2, T=1_024)\n",
    "\n",
    "torch.set_float32_matmul_precision(\"high\")\n",
    "\n",
    "# get logits\n",
    "model = GPT2(GPTConfig())\n",
    "model.to(device)\n",
    "model = torch.compile(model)\n",
    "\n",
    "# optimize\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=3e-4)\n",
    "for i in range(50):\n",
    "    t0 = time.time()\n",
    "    x, y = train_loader.next_batch()\n",
    "    x, y = x.to(device), y.to(device)\n",
    "    optimizer.zero_grad()\n",
    "    with torch.autocast(device_type=device, dtype=torch.float32):  # torch.bloat16 is slower for some reason and doesnt work with compile\n",
    "        logits, loss = model(x, y)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    if device == \"cuda\":\n",
    "        torch.cuda.synchronize()\n",
    "    t1 = time.time()\n",
    "    dt = (t1 - t0)*1000\n",
    "    tokens_per_sec = (train_loader.B * train_loader.T) / (t1 - t0)\n",
    "    print(f\"step {i}, loss: {loss.item()}, dt: {dt:.2f}ms, tok/sec: {tokens_per_sec:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f32ba3de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[01;34minput\u001b[0m/  \u001b[01;34mlib\u001b[0m/  \u001b[01;34mworking\u001b[0m/\n"
     ]
    }
   ],
   "source": [
    "ls"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
